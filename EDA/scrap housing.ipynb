{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from lxml import html\n",
    "import time\n",
    "import random\n",
    "from concurrent.futures import ThreadPoolExecutor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# List of proxies to use\n",
    "proxies = [\n",
    "    'http://mtsxqwli:s9cd501s0naq@207.244.217.165:6712',\n",
    "    'http://mtsxqwli:s9cd501s0naq@64.137.42.112:5157',\n",
    "    'http://mtsxqwli:s9cd501s0naq@161.123.152.115:6360',\n",
    "    'http://mtsxqwli:s9cd501s0naq@154.36.110.199:6853',\n",
    "    'http://mtsxqwli:s9cd501s0naq@173.0.9.70:5653',\n",
    "    'http://mtsxqwli:s9cd501s0naq@173.0.9.209:5792',\n",
    "    'http://mtsxqwli:s9cd501s0naq@107.172.163.27:6543',\n",
    "    'http://mtsxqwli:s9cd501s0naq@207.244.217.165:6712'\n",
    "]\n",
    "\n",
    "list_pages = []\n",
    "list_lxml = []\n",
    "\n",
    "# Function to scrape a single page\n",
    "def scrape_page(i):\n",
    "    url = f\"https://housing.com/in/buy/new_delhi/new_delhi?page={i}\"\n",
    "    \n",
    "    # Attempt to use proxies until one works\n",
    "    for proxy in proxies:\n",
    "        print(f\"Trying proxy: {proxy} for page {i}\")\n",
    "        try:\n",
    "            r = requests.get(url, proxies={\"http\": proxy, \"https\": proxy}, timeout=10)\n",
    "            if r.status_code == 200:\n",
    "                # Parse the page content using BeautifulSoup\n",
    "                soup = BeautifulSoup(r.text, 'lxml')\n",
    "                \n",
    "                # Convert the BeautifulSoup object to an lxml object\n",
    "                tree = html.fromstring(str(soup))\n",
    "                \n",
    "                # Append the page URL and the corresponding lxml tree\n",
    "                list_pages.append(url)\n",
    "                list_lxml.append(tree)\n",
    "                print(f\"Scraped {url} using proxy {proxy}\")\n",
    "                return 1  # Return 1 to indicate success\n",
    "            else:\n",
    "                print(f\"Failed to retrieve {url} with status code {r.status_code} using proxy {proxy}\")\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Error occurred while scraping {url} with proxy {proxy}: {e}\")\n",
    "    \n",
    "    # If no proxy worked, return 0 (failure)\n",
    "    return 0\n",
    "\n",
    "# Use ThreadPoolExecutor to scrape pages concurrently\n",
    "with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "    results = list(executor.map(scrape_page, range(2, 870)))\n",
    "\n",
    "# Print the total number of pages scraped\n",
    "total_scraped = sum(results)\n",
    "print(f\"Scraped {total_scraped} pages in total.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links_list = []\n",
    "for i in range(len(list_lxml)):\n",
    "    tree = list_lxml[i]  # Access the lxml tree for the first page\n",
    "    links = tree.xpath('//a[contains(@class, \"_j31f9d _c8dlk8 _g3l52n _csbfng _frwh2y T_e4485809 _ks15vq _vv1q9c _sq1l2s T_091c165f\")]/@href')\n",
    "    for x in links:\n",
    "        links_list.append(links)\n",
    "        print(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_list = []\n",
    "for i in links_list:\n",
    "    for x in i:\n",
    "        full_list.append(x)\n",
    "print(len(full_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_list = ['https://housing.com' + i for i in full_list]\n",
    "full_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicates(full_list):\n",
    "    seen = set()\n",
    "    unique_list = []\n",
    "    \n",
    "    for item in full_list:\n",
    "        if item not in seen:\n",
    "            unique_list.append(item)\n",
    "            seen.add(item)\n",
    "    \n",
    "    return unique_list\n",
    "\n",
    "# Example usage\n",
    "unique_list = remove_duplicates(full_list)\n",
    "print(len(unique_list))\n",
    "print(\"List after removing duplicates:\", unique_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resale = [i for i in unique_list if \"resale\" in i]\n",
    "print(len(resale))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import random\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import time\n",
    "import os\n",
    "\n",
    "# Function to extract property data based on headers\n",
    "def extract_property_data(soup, property_headers):\n",
    "    property_data = {header: None for header in property_headers}\n",
    "    for header in property_headers:\n",
    "        th_tag = soup.find('th', text=header)\n",
    "        if th_tag:\n",
    "            td_tag = th_tag.find_next_sibling('td')\n",
    "            if td_tag:\n",
    "                property_data[header] = td_tag.get_text(strip=True)\n",
    "    return property_data\n",
    "\n",
    "# Function to find distance for given category (school, hospital, etc.)\n",
    "def find_distance(soup, category):\n",
    "    category_div = soup.find('div', text=lambda t: t and category.lower() in t.lower())\n",
    "    if category_div:\n",
    "        distance_div = category_div.find_next('div', class_='_h31y44 _csbfng _c819bv _r3usic _vy1osq T_e080fff7')\n",
    "        if distance_div:\n",
    "            return distance_div.get_text(strip=True)\n",
    "    return None\n",
    "\n",
    "# Define headers for the property data\n",
    "property_headers = ['Price', 'Carpet Area', 'Bedrooms', 'Bathrooms', 'Parking', 'Balcony']\n",
    "\n",
    "# Specify the CSV file path\n",
    "csv_file = 'property_data.csv'\n",
    "\n",
    "# List of provided proxies\n",
    "proxies = [\n",
    "    'http://mtsxqwli:s9cd501s0naq@207.244.217.165:6712',\n",
    "    'http://mtsxqwli:s9cd501s0naq@64.137.42.112:5157',\n",
    "    'http://mtsxqwli:s9cd501s0naq@161.123.152.115:6360',\n",
    "    'http://mtsxqwli:s9cd501s0naq@154.36.110.199:6853',\n",
    "    'http://mtsxqwli:s9cd501s0naq@173.0.9.70:5653',\n",
    "    'http://mtsxqwli:s9cd501s0naq@173.0.9.209:5792',\n",
    "    'http://mtsxqwli:s9cd501s0naq@107.172.163.27:6543',\n",
    "    'http://mtsxqwli:s9cd501s0naq@207.244.217.165:6712'\n",
    "]\n",
    "\n",
    "# List of resale URLs to scrape\n",
    "\n",
    "\n",
    "# List of user-agent strings\n",
    "user_agents = [\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\",\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.159 Safari/537.36\",\n",
    "    # Add more user-agents as needed\n",
    "]\n",
    "\n",
    "# Function to check if URL is already scraped (exists in CSV)\n",
    "def is_url_scraped(url, csv_file):\n",
    "    if os.path.exists(csv_file):\n",
    "        df = pd.read_csv(csv_file)\n",
    "        if 'link' in df.columns and url in df['link'].values:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "# Function to scrape and save data for a given URL\n",
    "def scrape_property(url, proxies, csv_file):\n",
    "    try:\n",
    "        # Skip if URL has already been scraped\n",
    "        if is_url_scraped(url, csv_file):\n",
    "            print(f\"Skipping already scraped URL: {url}\")\n",
    "            return\n",
    "        \n",
    "        # Randomly select a user-agent and proxy\n",
    "        user_agent = random.choice(user_agents)\n",
    "        proxy = random.choice(proxies)  # Use a randomly selected proxy from the provided list\n",
    "        headers = {\"User-Agent\": user_agent}\n",
    "\n",
    "        # Make the request using the selected proxy\n",
    "        req = requests.get(url, headers=headers, proxies={\"http\": proxy, \"https\": proxy}, timeout=10)\n",
    "\n",
    "        # If the request is successful (status code 200)\n",
    "        if req.status_code == 200:\n",
    "            soup3 = BeautifulSoup(req.text, 'lxml')\n",
    "            data_flat = extract_property_data(soup3, property_headers)\n",
    "\n",
    "            # Extract additional information\n",
    "            data_flat['Location'] = soup3.find('div', class_='css-1ty5xzi').text if soup3.find('div', class_='css-1ty5xzi') else None\n",
    "            data_flat['Star_emi'] = soup3.find('a', class_=\"css-0\", attrs={'href': '/home-loans-emi-calculator'}).text if soup3.find('a', class_=\"css-0\", attrs={'href': '/home-loans-emi-calculator'}) else None\n",
    "            data_flat['school_distance'] = find_distance(soup3, 'School')\n",
    "            data_flat['hospital_distance'] = find_distance(soup3, 'Hospital')\n",
    "            data_flat['link'] = url\n",
    "            data_flat['About_property'] = soup3.find('div', class_='T_5255c66f _lorj18uv _v1ivgktf _g31tcg _7l9wsg _h3f6fq T_a3fd8ac3 _1ln11vji').get_text() if soup3.find('div', class_='T_5255c66f _lorj18uv _v1ivgktf _g31tcg _7l9wsg _h3f6fq T_a3fd8ac3 _1ln11vji') else None\n",
    "\n",
    "            # Save data to CSV\n",
    "            df_property_data = pd.DataFrame([data_flat])\n",
    "            df_property_data.to_csv(csv_file, mode='a', header=not pd.io.common.file_exists(csv_file), index=False)\n",
    "\n",
    "            print(f\"Data for {url} added to CSV.\")\n",
    "        else:\n",
    "            print(f\"Failed to retrieve data for {url} with status code: {req.status_code}\")\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error occurred while scraping {url}: {e}\")\n",
    "\n",
    "# Use ThreadPoolExecutor to parallelize scraping\n",
    "with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "    executor.map(lambda url: scrape_property(url, proxies, csv_file), resale)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proxy 154.36.110.199:6853 (Country: Germany) - Error: HTTPSConnectionPool(host='housing.com', port=443): Max retries exceeded with url: / (Caused by ProxyError('Unable to connect to proxy', OSError('Tunnel connection failed: 402 Payment Required')))\n",
      "Proxy 167.160.180.203:6754 (Country: United States) - Error: HTTPSConnectionPool(host='housing.com', port=443): Max retries exceeded with url: / (Caused by ProxyError('Unable to connect to proxy', OSError('Tunnel connection failed: 402 Payment Required')))\n",
      "Proxy 207.244.217.165:6712 (Country: Croatia) - Error: HTTPSConnectionPool(host='housing.com', port=443): Max retries exceeded with url: / (Caused by ProxyError('Unable to connect to proxy', OSError('Tunnel connection failed: 402 Payment Required')))\n",
      "Proxy 173.211.0.148:6641 (Country: United States) - Error: HTTPSConnectionPool(host='housing.com', port=443): Max retries exceeded with url: / (Caused by ProxyError('Unable to connect to proxy', OSError('Tunnel connection failed: 402 Payment Required')))\n",
      "Proxy 107.172.163.27:6543 (Country: United States) - Error: HTTPSConnectionPool(host='housing.com', port=443): Max retries exceeded with url: / (Caused by ProxyError('Unable to connect to proxy', OSError('Tunnel connection failed: 402 Payment Required')))\n",
      "Proxy 198.23.239.134:6540 (Country: United States) - Error: HTTPSConnectionPool(host='housing.com', port=443): Max retries exceeded with url: / (Caused by ProxyError('Unable to connect to proxy', OSError('Tunnel connection failed: 402 Payment Required')))\n",
      "Proxy 161.123.152.115:6360 (Country: Portugal) - Error: HTTPSConnectionPool(host='housing.com', port=443): Max retries exceeded with url: / (Caused by ProxyError('Unable to connect to proxy', OSError('Tunnel connection failed: 402 Payment Required')))\n",
      "Proxy 173.0.9.209:5792 (Country: United States) - Error: HTTPSConnectionPool(host='housing.com', port=443): Max retries exceeded with url: / (Caused by ProxyError('Unable to connect to proxy', OSError('Tunnel connection failed: 402 Payment Required')))\n",
      "Proxy 173.0.9.70:5653 (Country: United States) - Error: HTTPSConnectionPool(host='housing.com', port=443): Max retries exceeded with url: / (Caused by ProxyError('Unable to connect to proxy', OSError('Tunnel connection failed: 402 Payment Required')))\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# List of updated proxies\n",
    "proxies = [\n",
    "    {\"ip\": \"154.36.110.199\", \"port\": \"6853\", \"country\": \"Germany\", \"username\": \"mtsxqwli\", \"password\": \"s9cd501s0naq\"},\n",
    "    {\"ip\": \"167.160.180.203\", \"port\": \"6754\", \"country\": \"United States\", \"username\": \"mtsxqwli\", \"password\": \"s9cd501s0naq\"},\n",
    "    {\"ip\": \"207.244.217.165\", \"port\": \"6712\", \"country\": \"Croatia\", \"username\": \"mtsxqwli\", \"password\": \"s9cd501s0naq\"},\n",
    "    {\"ip\": \"173.211.0.148\", \"port\": \"6641\", \"country\": \"United States\", \"username\": \"mtsxqwli\", \"password\": \"s9cd501s0naq\"},\n",
    "    {\"ip\": \"107.172.163.27\", \"port\": \"6543\", \"country\": \"United States\", \"username\": \"mtsxqwli\", \"password\": \"s9cd501s0naq\"},\n",
    "    {\"ip\": \"198.23.239.134\", \"port\": \"6540\", \"country\": \"United States\", \"username\": \"mtsxqwli\", \"password\": \"s9cd501s0naq\"},\n",
    "    {\"ip\": \"161.123.152.115\", \"port\": \"6360\", \"country\": \"Portugal\", \"username\": \"mtsxqwli\", \"password\": \"s9cd501s0naq\"},\n",
    "    {\"ip\": \"173.0.9.209\", \"port\": \"5792\", \"country\": \"United States\", \"username\": \"mtsxqwli\", \"password\": \"s9cd501s0naq\"},\n",
    "    {\"ip\": \"173.0.9.70\", \"port\": \"5653\", \"country\": \"United States\", \"username\": \"mtsxqwli\", \"password\": \"s9cd501s0naq\"},\n",
    "]\n",
    "\n",
    "# URL to test\n",
    "url = \"https://housing.com/\"\n",
    "\n",
    "# Test each proxy\n",
    "for proxy in proxies:\n",
    "    proxy_url = f\"http://{proxy['username']}:{proxy['password']}@{proxy['ip']}:{proxy['port']}\"\n",
    "    proxies_dict = {\n",
    "        \"http\": proxy_url,\n",
    "        \"https\": proxy_url,\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Attempt to connect using the proxy\n",
    "        response = requests.get(url, proxies=proxies_dict, timeout=10)\n",
    "        \n",
    "        # Check if the connection is successful\n",
    "        if response.status_code == 200:\n",
    "            print(f\"Proxy {proxy['ip']}:{proxy['port']} (Country: {proxy['country']}) - Connection Successful\")\n",
    "        else:\n",
    "            print(f\"Proxy {proxy['ip']}:{proxy['port']} (Country: {proxy['country']}) - Failed with status code {response.status_code}\")\n",
    "    \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Proxy {proxy['ip']}:{proxy['port']} (Country: {proxy['country']}) - Error: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
